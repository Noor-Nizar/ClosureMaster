{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9017251,"sourceType":"datasetVersion","datasetId":4522584}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Kaggle Code Block","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nGITHUB_PAT = user_secrets.get_secret(\"GITHUB_PAT_CM\")\n\n!git clone \"https://username:{GITHUB_PAT}@github.com/Noor-Nizar/ClosureMaster.git\"\nos.chdir(\"ClosureMaster\")\n# !pip install -r 'requirements.txt' -q ## TODO add requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from models.PlaceNet import PlaceNet\nfrom helpers import logger, visualize_segmentation\nfrom datasets import SegmentationDataset\nimport logging\nimport torch\n\nlogger.setLevel(logging.INFO)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pn = PlaceNet()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_in_full = torch.ones((1,7,480,640))\ndummy_in_half = torch.ones((1,7,240,320))\ndummy_in_quarter = torch.ones((1,7,120,160))\n\nrecon_full, recon_half, recon_quarter = model_pn(dummy_in_full, dummy_in_half, dummy_in_quarter)\n\nprint(\"-\"*100)\nprint(recon_full.shape)\nprint(recon_half.shape)\nprint(recon_quarter.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helpers import WMSELoss\n\nloss = WMSELoss(recon_full, dummy_in_full)\nloss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\nfrom torch.utils.data import DataLoader\n\n# Initialize processor and model\nmodel_base = \"openmmlab/upernet-swin-large\"\nprocessor = AutoImageProcessor.from_pretrained(model_base)\nmodel = UperNetForSemanticSegmentation.from_pretrained(model_base)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace this with the path to your dataset of images\nimage_dir = \"/kaggle/input/city-center-visual-loop-detection/CityCentreImages/Images\"\n# image_dir = \"/Users/noornizar/LocalDocuments/ClosureMaster/images\"\n# \nimport glob\n\nall_images = glob.glob(image_dir + \"/*.jpg\")\n\n# Calculate the number of images for each split\nnum_images = len(all_images)\n# num_images = 16 ## for quick Testing  \n\nval_ratio = 0.2\ntest_ratio = 0.1\n\nnum_train = int(num_images * (1 - val_ratio - test_ratio))\nnum_val = int(num_images * val_ratio)\nnum_test = num_images - num_train - num_val\n\n# Shuffle the image paths\nimport random\nrandom.shuffle(all_images)\n\n# Split the image paths\ntraining_paths = all_images[:num_train]\nval_paths = all_images[num_train:num_train+num_val]\ntest_paths = all_images[num_train+num_val:]\n\nprint(f\"Number of training images: {len(training_paths)}\")\nprint(f\"Number of validation images: {len(val_paths)}\")\nprint(f\"Number of test images: {len(test_paths)}\")\n\n# Create datasets and dataloaders\ntrain_set = SegmentationDataset(training_paths, processor)\nval_set = SegmentationDataset(val_paths, processor)\ntest_set = SegmentationDataset(test_paths, processor)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_set, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=8, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=8, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helpers.utils import classify_objects, classify_objects_tensor_batched\nfrom helpers.visualization import convert_to_rgb, convert_to_rgb_batched","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s1, s2, s3 = next(iter(train_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\nmodel.eval()\n\nmodel_pn = model_pn.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as T\n\nnormalizer = T.Compose([\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n\nnormalizer_cls = T.Compose([\n                T.Normalize(mean=[0.485], std=[0.229])\n            ])\n\ndef dflow(pixel_values):\n    with torch.no_grad():\n        outputs = model(pixel_values)\n        batch_size = pixel_values.shape[0]\n        seg_list = processor.post_process_semantic_segmentation(outputs, target_sizes=[pixel_values.shape[2:]] * batch_size)\n        seg = torch.stack(seg_list)\n        cls_seg = classify_objects_tensor_batched(seg).unsqueeze(1)\n        rgb_seg = convert_to_rgb_batched(seg)\n    \n        \n        cls_seg = normalizer_cls(cls_seg)\n        \n        rgb_seg = rgb_seg.float()/255\n        rgb_seg = normalizer(rgb_seg)\n        \n        combined = torch.cat([pixel_values, cls_seg, rgb_seg], dim=1)\n    return combined","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process each image in the dataset\ns1, s2, s3 = next(iter(train_loader))\ns1 = s1.to(device)\ns2 = s2.to(device)\ns3 = s3.to(device)\n\ncombined_s1 = dflow(s1)    \ncombined_s2 = dflow(s2)\ncombined_s3 = dflow(s3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_s1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_s2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_s3.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recon_full, recon_half, recon_quarter = model_pn(combined_s1, combined_s2, combined_s3)\n\nprint(\"-\"*100)\nprint(recon_full.shape)\nprint(recon_half.shape)\nprint(recon_quarter.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ... existing imports ...\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\n\n%matplotlib inline\n\ndef train_model(model_pn, train_loader, val_loader, num_epochs=20, patience=10, lr=0.3):\n    \n    optimizer = Adam(model_pn.parameters(), lr=lr)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=True)\n    criterion = WMSELoss\n\n    best_val_loss = float('inf')\n    early_stopping_counter = 0\n    \n    # Lists to store metrics for plotting\n    train_losses = []\n    val_losses = []\n    learning_rates = []\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model_pn.train()\n        train_loss = 0.0\n        \n        for s1, s2, s3 in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n            s1, s2, s3 = s1.to(device), s2.to(device), s3.to(device)\n            \n            optimizer.zero_grad()\n            \n            combined_s1 = dflow(s1)\n            combined_s2 = dflow(s2)\n            combined_s3 = dflow(s3)\n            \n            recon_full, recon_half, recon_quarter = model_pn(combined_s1, combined_s2, combined_s3)\n            \n            loss = criterion(recon_full, combined_s1) + \\\n                   criterion(recon_half, combined_s2) + \\\n                   criterion(recon_quarter, combined_s3)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Validation phase\n        model_pn.eval()\n        val_loss = 0.0\n        \n        with torch.no_grad():\n            for s1, s2, s3 in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n                s1, s2, s3 = s1.to(device), s2.to(device), s3.to(device)\n                \n                combined_s1 = dflow(s1)\n                combined_s2 = dflow(s2)\n                combined_s3 = dflow(s3)\n                \n                recon_full, recon_half, recon_quarter = model_pn(combined_s1, combined_s2, combined_s3)\n                \n                loss = criterion(recon_full, combined_s1) + \\\n                       criterion(recon_half, combined_s2) + \\\n                       criterion(recon_quarter, combined_s3)\n                \n                val_loss += loss.item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        \n        # Get current learning rate\n        current_lr = optimizer.param_groups[0]['lr']\n        learning_rates.append(current_lr)\n    \n        # Learning rate scheduler step\n        scheduler.step(avg_val_loss)\n        \n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            early_stopping_counter = 0\n            # Save the best model\n            torch.save(model_pn.state_dict(), \"best_placenet_model.pth\")\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\n                break\n        \n        clear_output()\n        \n        # Print epoch results\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n        \n        # Plot training and validation loss\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.plot(train_losses, label='Training Loss')\n        plt.plot(val_losses, label='Validation Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training and Validation Loss')\n        plt.legend()\n\n        # Plot learning rate\n        plt.subplot(1, 2, 2)\n        plt.plot(learning_rates)\n        plt.xlabel('Epoch')\n        plt.ylabel('Learning Rate')\n        plt.title('Learning Rate over Epochs')\n        plt.yscale('log')  # Use log scale for better visualization\n\n        plt.tight_layout()\n        plt.savefig('training_metrics.png')\n        plt.show()\n    \n    return model_pn, train_losses, val_losses, learning_rates\n\n# Usage\n# model_pn = PlaceNet()\ntrained_model, train_losses, val_losses, learning_rates = train_model(model_pn, train_loader, val_loader, num_epochs=50, patience=10, lr=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    s1, s2, s3 = next(iter(train_loader))\n\n    s1 = s1.to(device)\n    s2 = s2.to(device)\n    s3 = s3.to(device)\n\n    combined_s1 = dflow(s1)    \n    combined_s2 = dflow(s2)\n    combined_s3 = dflow(s3)\n\n    recon_full, recon_half, recon_quarter = model_pn(combined_s1, combined_s2, combined_s3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recon_quarter_np = recon_quarter.detach().cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_orig = s3.detach().cpu().numpy()[0]\nvis_orig = np.transpose(vis_orig, (1,2,0))\nplt.imshow(vis_orig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis = np.transpose(recon_quarter_np[0][0:3], (1,2,0))\n\nplt.imshow(vis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NICE","metadata":{}}]}